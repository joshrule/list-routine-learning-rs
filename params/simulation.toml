[simulation]

# The location of the signature.
signature = "params/signature.txt"

# The location of the background knowledge.
background = "params/background.txt"

# Make this many predictions.
n_predictions = 11
# n_predictions = 8
# n_predictions = 5
# n_predictions = 2

# Search for at most this many seconds for each datum.
# timeout = 1200
# timeout = 600
# timeout = 300
# timeout = 180
# timeout = 120
# timeout = 90
timeout = 60
# timeout = 30
# timeout = 10
# timeout = 5
# timeout = 2
# timeout = 1

# Scale generations_per_datum by this factor after each prediction (multiply
# for good performance, divide for bad performance).
confidence = 1.0

# Only learn deterministic TRSs.
deterministic = true

# Bound numbers:
lo = 0
# hi = 99
hi = 9

# How many hypotheses do we preserve across trials?
top_n = 100

# What's the likelihood temperature?
temperature = 1.0

# What's the temperature of the TRS prior vs. the Metaprogram prior?
trs_temperature = 10.0

[mcts]

# The maximum number of search moves that can be chained together.
max_depth = 50

# The maximum number of states that can be searched.
max_states = 1000000000

max_revisions = 7

max_size = 100

invent = true

atom_weights = [0.8, 1.0]

selection = { "BestInSubtree" = { 0 = 5, 1 = 1.5 } }

[model]

# The reported posterior is (log_prior * p_temp + log_likelihood * l_temp) / schedule(t).
p_temp = 1.0
l_temp = 1.0

# We don't anneal at all: schedule(t) = 1.0.
schedule = "None"

# The prior is generative: sample the number of rules and then sample each rule up to that number.
prior = { "SimpleGenerative" = { p_rule = 0.5, atom_weights = [0.8, 1.0] } }

# The likelihood is generative: rewrite the observed input according to your
# hypothesis and scale the probability of each computed output by the cost of
# probabilistically corrupting it into the observed output.
#
# For the well-formedness likelihood on novel inputs, the probability, p, of
# each element, e, in the trace is adjusted to:
# - p*0.9999 if e is a literal list (i.e. -0.0001 in ln-space)
# - p*0.0001 if e is not a literal list (i.e. -9.2 in ln-space)
#
# alpha: This is the probability that we sampled an output rather than getting it by rewriting.
[model.likelihood]
decay = 0.0
strategy = "Normal"
representation = { logic = "Symbolic", app = "NonApplicative" }
p_observe = 0.0
max_steps = 120
max_depth = 0
max_size = 200
single = { "ListPrefix" = { alpha = 0.000001, p_add = 0.0001, p_del = 0.0001 } }
