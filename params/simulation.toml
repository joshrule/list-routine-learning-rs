[simulation]

# Print debugging information?
verbose = true
# verbose = false

# The location of the signature.
signature = "params/signature.txt"

# The location of the background knowledge.
background = "params/background.txt"

# Make this many predictions.
n_predictions = 11
# n_predictions = 8

# Search for at most this many seconds for each datum.
# timeout = 28800
# timeout = 7200
# timeout = 3600
# timeout = 1800
# timeout = 1200
# timeout = 900
# timeout = 600
# timeout = 300
# timeout = 180
# timeout = 120
# timeout = 90
# timeout = 60
# timeout = 30
timeout = 20
# timeout = 10
# timeout = 5
# timeout = 2
# timeout = 1
# timeout = 0

steps = 1000
# steps = 0

# Only learn deterministic TRSs.
deterministic = true

# Bound numbers:
lo = 0
# hi = 99
hi = 9

# How many hypotheses do we preserve across trials?
top_n = 100

# p_refactor = 1.0
# p_refactor = 0.5
p_refactor = 0.0

# mode = "Batch"
mode = "Online"

[model]

# The reported posterior is (log_prior + log_likelihood) / schedule(t).
# We don't anneal at all: schedule(t) = 1.0.
schedule = "None"

# The prior is generative: sample the number of rules and then sample each rule up to that number.
prior = { "SimpleGenerative" = { p_rule = 0.5, w_var = 5.0 } }

lesion = "None"

# The likelihood is generative: rewrite the observed input according to your
# hypothesis and scale the probability of each computed output by the cost of
# probabilistically corrupting it into the observed output.
#
# For the well-formedness likelihood on novel inputs, the probability, p, of
# each element, e, in the trace is adjusted to:
# - p*0.9999 if e is a literal list (i.e. -0.0001 in ln-space)
# - p*0.0001 if e is not a literal list (i.e. -9.2 in ln-space)
[model.likelihood]
decay = 0.0
strategy = "Normal"
representation = { logic = "Symbolic", app = "NonApplicative" }
p_observe = 0.0
max_steps = 120
max_depth = 0
max_size = 200
single = { "ListPrefix" = { p_add = 1e-4, p_del = 1e-4 } }
