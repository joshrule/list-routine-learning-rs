[simulation]

# Make this many predictions.
n_predictions = 11

# Search for at most this many seconds for each datum.
# timeout = 600
timeout = 5

# Scale generations_per_datum by this factor after each prediction (multiply
# for good performance, divide for bad performance).
confidence = 0.5

# Only learn deterministic TRSs.
deterministic = true

[mcts]

# The maximum number of search moves that can be chained together.
max_depth = 1000

# The maximum number of states that can be searched.
max_states = 1000000000

max_revisions = 5

max_size = 5

invent = true

atom_weights = [1.0, 1.0, 1.0, 1.0]

# The moves used during search
moves = [
    "Variablize",
    "Generalize",
    "Compose",
    "MemorizeOne",
    "Memorize",
    "Recurse",
    "DeleteRules",
    "SampleRule",
    "RegenerateRule",
    ]

[model]

# The reported posterior is (log_prior * p_temp + log_likelihood * l_temp) / schedule(t).
p_temp = 0.01
l_temp = 1.0

# We don't anneal at all: schedule(t) = 1.0.
schedule = "None"

# The prior is generative: sample the number of rules and then sample each rule up to that number.
prior = { "SimpleGenerative" = { p_rule = 0.5, atom_weights = [1.5, 1.5, 1.0, 1.5] } }

# The likelihood is generative: rewrite the observed input according to your
# hypothesis and scale the probability of each computed output by the cost of
# probabilistically corrupting it into the observed output.
#
# It's annoying, but you need to manually normalize the parameters so that:
# - p_insertion = weight1/n_chars
# - p_incorrect_sub = weight2/n_chars
# - p_deletion + weight2 + p_correct_sub = 1.0
likelihood = { decay = 0.9, strategy = "Normal", p_observe= 0.0, max_steps = 25, max_depth = 25, max_size = 200, single = { "List" = { t_max = 10, d_max = 10, dist = { beta = 0.005, p_insertion = 0.001, p_deletion = 0.0025, p_incorrect_sub = { "Bounded" = { low = 0, high = 99, weight = 0.0025}}, p_correct_sub = 0.995 }}}}
