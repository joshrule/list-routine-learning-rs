[simulation]

# Make this many predictions.
n_predictions = 11

# Search for at most this many seconds for each datum.
timeout = 600

# Scale generations_per_datum by this factor after each prediction (multiply
# for good performance, divide for bad performance).
confidence = 0.5

[genetic]

# Maximum length of a rule sampled during genesis.
max_sample_size = 10 

# Per-symbol grammar weights used during genesis.
atom_weights = [
    1.5, # existing variables
    1.5, # operators with arity 0
    1.0, # operators with arity > 0
    1.5  # invented variables
    ]

# If true, genesis (and subsequent learning) produces deterministic TRSs.
deterministic = true

# How many times can search recurse?
depth = 0

# The moves used during reproduction.
moves = [
    {weight = 2, mv =   "Variablize"                                              },
    {weight = 2, mv =   "Generalize"                                              },
    {weight = 2, mv = { "Recurse"        = 8 }                                    },
    {weight = 2, mv =   "Compose"                                                 },
    {weight = 1, mv = { "DeleteRules"    = 8 }                                    },
    {weight = 1, mv =   "MemorizeOne"                                             },
    {weight = 1, mv = { "SampleRule"     = { 0 = [1.5, 1.5, 1.0, 1.5], 1 = 10 } } },
    {weight = 1, mv = { "RegenerateRule" = { 0 = [1.5, 1.5, 1.0, 1.5], 1 = 10 } } },
    ]

[gp]

# Select individuals after reproduction in this way.
# Deterministically maintain 1/8 of population as the best, sample the rest.
selection = { "Hybrid" = 0.125 }

# Maintain a population of this size.
population_size = 16 

# Run tournaments of this many individuals.
tournament_size = 1

# Weight tournaments.
weighted = true

# Generate n_delta children during each generation.
n_delta = 8

[model]

# The reported posterior is (log_prior * p_temp + log_likelihood * l_temp) / schedule(t).
p_temp = 0.01
l_temp = 1.0

# We don't anneal at all: schedule(t) = 1.0.
schedule = "None"

# The prior is generative: sample the number of rules and then sample each rule up to that number.
prior = { "SimpleGenerative" = { p_rule = 0.5, atom_weights = [1.5, 1.5, 1.0, 1.5] } }

# The likelihood is generative: rewrite the observed input according to your
# hypothesis and scale the probability of each computed output by the cost of
# probabilistically corrupting it into the observed output.
#
# It's annoying, but you need to manually normalize the parameters so that:
# - p_insertion = weight1/n_chars
# - p_incorrect_sub = weight2/n_chars
# - p_deletion + weight2 + p_correct_sub = 1.0
likelihood = { decay = 0.9, strategy = "Normal", p_observe= 0.0, max_steps = 25, max_depth = 25, max_size = 200, single = { "List" = { t_max = 10, d_max = 10, dist = { beta = 0.005, p_insertion = 0.001, p_deletion = 0.0025, p_incorrect_sub = { "Bounded" = { low = 0, high = 99, weight = 0.0025}}, p_correct_sub = 0.995 }}}}
